{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#the-autoembedder","title":"The Autoembedder","text":"<p>          </p>"},{"location":"#introduction","title":"Introduction","text":"<p>The Autoembedder is an autoencoder with additional embedding layers for the categorical columns. Its usage is flexible, and hyperparameters like the number of layers can be easily adjusted and tuned. The data provided for training can be either a path to a Dask or Pandas DataFrame stored in the Parquet format or the DataFrame object directly.</p>"},{"location":"#installation","title":"Installation","text":"<p>If you are using Poetry, you can install the package with the following command: <pre><code>poetry add autoembedder\n</code></pre> If you are using pip, you can install the package with the following command: <pre><code>pip install autoembedder\n</code></pre></p>"},{"location":"#installing-dependencies","title":"Installing dependencies","text":"<p>With Poetry: <pre><code>poetry install\n</code></pre> With pip: <pre><code>pip install -r requirements.txt\n</code></pre></p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#0-some-imports","title":"0. Some imports","text":"<pre><code>from autoembedder import Autoembedder, dataloader, fit\n</code></pre>"},{"location":"#1-create-dataloaders","title":"1. Create dataloaders","text":"<p>First, we create two <code>dataloaders</code>. One for training, and the other for validation data. As <code>source</code> they either accept a path to a Parquet file, to a folder of Parquet files or a Pandas/Dask DataFrame. <pre><code>train_dl = dataloader(train_df)\nvalid_dl = dataloader(vaild_df)\n</code></pre></p>"},{"location":"#2-set-parameters","title":"2. Set parameters","text":"<p>Now, we need to set the parameters. They are going to be used for handling the data and training the model. In this example, only parameters for the training are set. Here you find a list of all possible parameters. This should do it: <pre><code>parameters = {\n    \"hidden_layers\": [[25, 20], [20, 10]],\n    \"epochs\": 10,\n    \"lr\": 0.0001,\n    \"verbose\": 1,\n}\n</code></pre></p>"},{"location":"#3-initialize-the-autoembedder","title":"3. Initialize the autoembedder","text":"<p>Then, we need to initialize the autoembedder. In this example, we are not using any categorical features. So we can skip the <code>embedding_sizes</code> argument. <pre><code>model = Autoembedder(parameters, num_cont_features=train_df.shape[1])\n</code></pre></p>"},{"location":"#4-train-the-model","title":"4. Train the model","text":"<p>Everything is set up. Now we can fit the model. <pre><code>fit(parameters, model, train_dl, valid_dl)\n</code></pre></p>"},{"location":"#example","title":"Example","text":"<p>Check out this Jupyter notebook for an applied example using the Credit Card Fraud Detection from Kaggle.</p>"},{"location":"#parameters","title":"Parameters","text":"<p>This is a list of all parameters that can be passed to the Autoembedder for training. When using the training script the <code>_</code> needs to be replaced with <code>-</code> and the parameters need to be passed as arguments. For boolean values please have a look at the <code>Comment</code> column for understanding how to pass them.</p>"},{"location":"#run-the-training-script","title":"Run the training script","text":"<p>You can also simply use the training script:: <pre><code>python3 training.py \\\n--epochs 20 \\\n--train-input-path \"path/to/your/train_data\" \\\n--test-input-path \"path/to/your/test_data\" \\\n--hidden-layers \"[[12, 6], [6, 3]]\"\n</code></pre></p> <p>for help just run: <pre><code>python3 training.py --help\n</code></pre></p>    Argument Type Required Default value Comment     batch_size int False 32    drop_last bool False True --drop-last / --no-drop-last   pin_memory bool False True --pin-memory / --no-pin-memory   num_workers int False 0 0 means that the data will be loaded in the main process   use_mps bool False False --use-mps / --no-use-mps   model_title str False autoembedder_{<code>datetime</code>}.bin    model_save_path str False     n_save_checkpoints int False     lr float False 0.001    amsgrad bool False False --amsgrad / --no-amsgrad   epochs int True     dropout_rate float False 0 Dropout rate for the dropout layers in the encoder and decoder.   layer_bias bool False True --layer-bias / --no-layer-bias   weight_decay float False False    l1_lambda float False 0    xavier_init bool False False --xavier-init / --no-xavier-init   activation str False tanh Activation function; either <code>tanh</code>, <code>relu</code>, <code>leaky_relu</code> or <code>elu</code>   tensorboard_log_path str False     trim_eval_errors bool False False --trim-eval-errors / --no-trim-eval-errors; Removes the max and min loss when calculating the <code>mean loss diff</code> and <code>median loss diff</code>. This can be useful if some rows create very high losses.   verbose int False 0 Set this to <code>1</code> if you want to see the model summary and the validation and evaluation results. set this to <code>2</code> if you want to see the training progress bar. <code>0</code> means no output.   target str False  The target column. If not set no evaluation will be performed.   train_input_path str True     test_input_path str True     eval_input_path str False  Path to the evaluation data. If no path is provided no evaluation will be performed.   hidden_layers str True  Contains a string representation of a list of list of integers which represents the hidden layer structure. E.g.: <code>\"[[64, 32], [32, 16], [16, 8]]\"</code> activation   cat_columns str False \"[]\" Contains a string representation of a list of list of categorical columns (strings). The columns which use the same encoder should be together in a list. E.g.: <code>\"[['a', 'b'], ['c']]\"</code>.   drop-cat-columns bool False  --drop-cat-columns / --no-drop-cat-columns"},{"location":"#why-additional-embedding-layers","title":"Why additional embedding layers?","text":"<p>The additional embedding layers automatically embed all columns with the Pandas <code>category</code> data type. If categorical columns have another data type, they will not be embedded and will be handled like continuous columns. Simply encoding the categorical values (e.g., with the usage of a label encoder) decreases the quality of the outcome.</p>"},{"location":"CONTRIBUTING/","title":"How to contribute","text":""},{"location":"CONTRIBUTING/#poetry","title":"Poetry","text":"<p>We are using Poetry to manage the dependencies, for deployment, and the virtual environment. If you have not used it before please check out the documentation to get started.</p> <p>If you want to start working on the project. The first thing you have to do is: <pre><code>poetry install\n</code></pre> This installs all needed dependencies for development.</p>"},{"location":"CONTRIBUTING/#pre-commit-hooks","title":"Pre-commit hooks","text":"<p>We are using pre-commit to ensure a consistent code style and to avoid common mistakes. Please install the pre-commit and install the hook with: <pre><code>pre-commit install --hook-type commit-msg\n</code></pre></p>"},{"location":"CONTRIBUTING/#homebrew","title":"Homebrew","text":"<p>We are using Homebrew to manage the dependencies for the development environment. Please install Homebrew and run <pre><code> brew bundle\n</code></pre> to install the dependencies. If you don't want/can't use Homebrew, you can also install the dependencies manually.</p>"},{"location":"CONTRIBUTING/#conventional-commits","title":"Conventional Commits","text":"<p>We are using Conventional Commits to ensure a consistent commit message style. Please use the following commit message format: <pre><code>&lt;type&gt;[optional scope]: &lt;description&gt;\n</code></pre></p> <p>E.g.: <pre><code>feat: add a new fantastic feature \ud83d\ude80\n</code></pre></p>"},{"location":"CONTRIBUTING/#how-to-contribute_1","title":"How to contribute","text":"<p>The following steps will give a short guide on how to contribute to this project:</p> <ul> <li>Create a personal fork of the project on GitHub.</li> <li>Clone the fork on your local machine. Your remote repo on GitHub is called <code>origin</code>.</li> <li>Add the original repository as a remote called <code>upstream</code>.</li> <li>If you created your fork a while ago be sure to pull upstream changes into your local repository.</li> <li>Create a new branch to work on! Start from <code>develop</code> if it exists, else from <code>main</code>.</li> <li>Implement/fix your feature, comment your code, and add some examples.</li> <li>Follow the code style of the project, including indentation. Black, isort, Pylint, and mypy can help you with it.</li> <li>Run all tests.</li> <li>Write or adapt tests as needed.</li> <li>Add or change the documentation as needed. Please follow the \"Google Python Style Guide\".</li> <li>Squash your commits into a single commit with git's interactive rebase. Create a new branch if necessary.</li> <li>Push your branch to your fork on GitHub, the remote <code>origin</code>.</li> <li>From your fork open a pull request in the correct branch. Target the project's <code>develop</code> branch!</li> <li>Once the pull request is approved and merged you can pull the changes from <code>upstream</code> to your local repo and delete your extra branch(es).</li> </ul>"},{"location":"data/","title":"Data","text":""},{"location":"data/#src.autoembedder.data.dataloader","title":"<code>dataloader(source, parameters=None)</code>","text":"<p>Parameters:</p>    Name Type Description Default     <code>source</code>  <code>Union[str, dask.DataFrame, pandas.DataFrame]</code>  <p>Path to Dask/Pandas DataFrame stored as Parquet or a Dask/Pandas DataFrame.</p>  required    <code>parameters</code>  <code>Optional[Dict[str, Any]]</code>  <p>Parameters for the DataLoader. In the documentation all possible parameters are listed.</p>  <code>None</code>     <p>Returns:</p>    Type Description      <code>DataLoader</code>  <p>torch.utils.data.DataLoader: A DataLoader object</p>     Source code in <code>src/autoembedder/data.py</code> <pre><code>def dataloader(\n    source: Union[str, dd.DataFrame, pd.DataFrame],\n    parameters: Optional[Dict[str, Any]] = None,\n) -&gt; DataLoader:\n    \"\"\"\n    Args:\n        source (Union[str, dask.DataFrame, pandas.DataFrame]): Path to Dask/Pandas DataFrame stored as Parquet or a Dask/Pandas DataFrame.\n        parameters (Optional[Dict[str, Any]]): Parameters for the DataLoader.\n            In the [documentation](https://chrislemke.github.io/autoembedder/#parameters) all possible parameters are listed.\n\n    Returns:\n        torch.utils.data.DataLoader: A DataLoader object\n    \"\"\"\n    if parameters is None:\n        parameters = {}\n\n    return DataLoader(\n        dataset=_Dataset(source, parameters.get(\"drop_cat_columns\", False)),\n        batch_size=parameters.get(\"batch_size\", 32),\n        pin_memory=parameters.get(\"pin_memory\", True),\n        num_workers=parameters.get(\"num_workers\", 0),\n        drop_last=parameters.get(\"drop_last\", True),\n    )\n</code></pre>"},{"location":"evaluator/","title":"Evaluator","text":""},{"location":"evaluator/#src.autoembedder.evaluator.loss_delta","title":"<code>loss_delta(_, __, model, parameters, df=None)</code>","text":"<p>This evaluation function calculates the loss delta between the training and test set. This delta describes how well the model can distinguish between the categories of the target variable.</p> <p>Parameters:</p>    Name Type Description Default     <code>_</code>  <code>None</code>  <p>Not in use. Needed by Pytorch-ignite.</p>  required    <code>__</code>  <code>None</code>  <p>Not in use. Needed by Pytorch-ignite.</p>  required    <code>model</code>  <code>Autoembedder</code>  <p>Instance from the model used for prediction.</p>  required    <code>parameters</code>  <code>Dict[str, Any]</code>  <p>Dictionary with the parameters used for training and prediction. In the documentation all possible parameters are listed.</p>  required    <code>df</code>  <code>Optional[Union[dd.DataFrame, pd.DataFrame]]</code>  <p>Dask or Pandas DataFrame. If it is not given, the data is loaded from the given path (<code>eval_input_path</code>).</p>  <code>None</code>     <p>Returns:</p>    Type Description      <code>Tuple[float, float]</code>  <p>Tuple[float, float]: <code>loss_mean_diff</code>, <code>loss_std_diff</code> and DataFrame.</p>     Source code in <code>src/autoembedder/evaluator.py</code> <pre><code>def loss_delta(_, __, model: Autoembedder, parameters: Dict[str, Any], df: Optional[Union[dd.DataFrame, pd.DataFrame]] = None) -&gt; Tuple[float, float]:  # type: ignore\n    \"\"\"This evaluation function calculates the loss delta between the training\n    and test set. This delta describes how well the model can distinguish\n    between the categories of the target variable.\n\n    Args:\n        _ (None): Not in use. Needed by Pytorch-ignite.\n        __ (None): Not in use. Needed by Pytorch-ignite.\n        model (Autoembedder): Instance from the model used for prediction.\n        parameters (Dict[str, Any]): Dictionary with the parameters used for training and prediction.\n            In the [documentation](https://chrislemke.github.io/autoembedder/#parameters) all possible parameters are listed.\n        df (Optional[Union[dd.DataFrame, pd.DataFrame]], optional): Dask or Pandas DataFrame. If it is not given,\n            the data is loaded from the given path (`eval_input_path`).\n\n    Returns:\n        Tuple[float, float]: `loss_mean_diff`, `loss_std_diff` and DataFrame.\n    \"\"\"\n    if parameters.get(\"eval_input_path\", None) is not None:\n        try:\n            df = (\n                dd.read_parquet(parameters[\"eval_input_path\"], infer_divisions=True)\n                .compute()\n                .sample(frac=1)\n            )\n        except ValueError:\n            df = pd.read_parquet(parameters[\"eval_input_path\"]).sample(frac=1)\n    elif df is not None:\n        if isinstance(df, dd.DataFrame):\n            df = df.compute()\n        df = df.sample(frac=1)\n    else:\n        raise ValueError(\n            \"No DataFrame given! Please provide a DataFrame or a path to a parquet file.\"\n        )\n\n    target = parameters[\"target\"]\n\n    df_1 = df.query(f\"{target} == 1\").drop([target], axis=1)\n    df_0 = df.query(f\"{target} == 0\").drop([target], axis=1).sample(n=df_1.shape[0])\n\n    losses_0: List[float] = []\n    losses_1: List[float] = []\n\n    for losses_df, losses in [(df_0, losses_0), (df_1, losses_1)]:\n        loss = MSELoss()\n        for batch in losses_df.itertuples(index=False):\n            losses.append(_predict(model, batch, loss, parameters))\n\n    if parameters.get(\"trim_eval_errors\", 0) == 1:\n        losses_0.remove(max(losses_0))\n        losses_0.remove(min(losses_0))\n        losses_1.remove(max(losses_1))\n        losses_1.remove(min(losses_1))\n\n    return np.absolute(np.mean(losses_1) - np.mean(losses_0)), np.absolute(\n        np.median(losses_1) - np.median(losses_0)\n    )\n</code></pre>"},{"location":"learner/","title":"Learner","text":""},{"location":"learner/#src.autoembedder.learner.__training_step","title":"<code>__training_step(engine, batch, model, optimizer, criterion, parameters)</code>","text":"<p>Here the actual training step is performed. It is called by the training engine. Not using PyTorch ignite this code would be wrapped in some kind of training loop over a range of epochs and batches. But using ignite this is handled by the engine.</p> <p>Parameters:</p>    Name Type Description Default     <code>engine</code>  <code>ignite.engine.Engine</code>  <p>The engine that is calling this method.</p>  required    <code>batch</code>  <code>NamedTuple</code>  <p>The batch that is passed to the engine for training.</p>  required    <code>model</code>  <code>Autoembedder</code>  <p>The model to be trained.</p>  required    <code>optimizer</code>  <code>torch.optim</code>  <p>The optimizer to be used for training.</p>  required    <code>criterion</code>  <code>torch.nn.MSELoss</code>  <p>The loss function to be used for training.</p>  required    <code>parameters</code>  <code>Dict[str, Any]</code>  <p>The parameters of the training process.</p>  required     <p>Returns:</p>    Type Description      <code>Union[np.float32, np.float64]</code>  <p>Union[np.float32, np.float64]: The loss of the current batch.</p>     Source code in <code>src/autoembedder/learner.py</code> <pre><code>def __training_step(\n    engine: Engine,\n    batch: NamedTuple,\n    model: Autoembedder,\n    optimizer: Adam,\n    criterion: MSELoss,\n    parameters: Dict,\n) -&gt; Union[np.float32, np.float64]:\n    \"\"\"Here the actual training step is performed. It is called by the training\n    engine. Not using [PyTorch ignite](https://github.com/pytorch/ignite) this\n    code would be wrapped in some kind of training loop over a range of epochs\n    and batches. But using ignite this is handled by the engine.\n\n    Args:\n        engine (ignite.engine.Engine): The engine that is calling this method.\n        batch (NamedTuple): The batch that is passed to the engine for training.\n        model (Autoembedder): The model to be trained.\n        optimizer (torch.optim): The optimizer to be used for training.\n        criterion (torch.nn.MSELoss): The loss function to be used for training.\n        parameters (Dict[str, Any]): The parameters of the training process.\n\n    Returns:\n        Union[np.float32, np.float64]: The loss of the current batch.\n    \"\"\"\n\n    model.train()\n    optimizer.zero_grad()\n    cat, cont = model_input(batch, parameters)\n    outputs = model(cat, cont)\n    train_loss = criterion(outputs, model.last_target)\n\n    if parameters.get(\"l1_lambda\", 0) &gt; 0:\n        l1_lambda = parameters[\"l1_lambda\"]\n        l1_norm = sum(p.abs().sum() for p in model.parameters())\n        train_loss = train_loss + l1_lambda * l1_norm\n\n    train_loss.backward()\n    optimizer.step()\n    return train_loss.item()\n</code></pre>"},{"location":"learner/#src.autoembedder.learner.__validation_step","title":"<code>__validation_step(engine, batch, model, criterion, parameters)</code>","text":"<p>Parameters:</p>    Name Type Description Default     <code>engine</code>  <code>ignite.engine.Engine</code>  <p>The engine that is calling this method.</p>  required    <code>batch</code>  <code>NamedTuple</code>  <p>The batch that is passed to the engine for validation.</p>  required    <code>model</code>  <code>Autoembedder</code>  <p>The model used for validation.</p>  required    <code>criterion</code>  <code>torch.nn.MSELoss</code>  <p>The loss function to be used for validation.</p>  required    <code>parameters</code>  <code>Dict[str, Any]</code>  <p>The parameters of the validation process.</p>  required     <p>Returns:</p>    Type Description      <code>Union[np.float32, np.float64]</code>  <p>Union[np.float32, np.float64]: The loss of the current batch.</p>     Source code in <code>src/autoembedder/learner.py</code> <pre><code>def __validation_step(\n    engine: Engine,\n    batch: NamedTuple,\n    model: Autoembedder,\n    criterion: MSELoss,\n    parameters: Dict,\n) -&gt; Union[np.float32, np.float64]:\n    \"\"\"\n    Args:\n        engine (ignite.engine.Engine): The engine that is calling this method.\n        batch (NamedTuple): The batch that is passed to the engine for validation.\n        model (Autoembedder): The model used for validation.\n        criterion (torch.nn.MSELoss): The loss function to be used for validation.\n        parameters (Dict[str, Any]): The parameters of the validation process.\n\n    Returns:\n        Union[np.float32, np.float64]: The loss of the current batch.\n    \"\"\"\n\n    model.eval()\n    with torch.no_grad():\n        cat, cont = model_input(batch, parameters)\n        val_outputs = model(cat, cont)\n        val_loss = criterion(val_outputs, model.last_target)\n    return val_loss.item()\n</code></pre>"},{"location":"learner/#src.autoembedder.learner.fit","title":"<code>fit(parameters, model, train_dataloader, test_dataloader, eval_df=None)</code>","text":"<p>This method is the general wrapper around the fitting process. It is preparing the optimizer, the loss function, the trainer, the validator and the evaluator. Then it attaches everything to the corresponding engines and runs the training.</p> <p>Parameters:</p>    Name Type Description Default     <code>parameters</code>  <code>Dict[str, Any]</code>  <p>The parameters of the training process. In the documentation all possible parameters are listed.</p>  required    <code>model</code>  <code>Autoembedder</code>  <p>The model to be trained.</p>  required    <code>train_dataloader</code>  <code>torch.utils.data.DataLoader</code>  <p>The dataloader for the training data.</p>  required    <code>test_dataloader</code>  <code>torch.utils.data.DataLoader</code>  <p>The dataloader for the test data.</p>  required    <code>eval_df</code>  <code>Optional[Union[dd.DataFrame, pd.DataFrame]]</code>  <p>Dask or Pandas DataFrame for the evaluation step. If the path to the evaluation data is given in the parameters (<code>eval_input_path</code>) this argument is not needed. If neither <code>eval_input_path</code> nor <code>eval_df</code> is given, no evaluation step is performed.</p>  <code>None</code>     <p>Returns:</p>    Name Type Description     <code>Autoembedder</code>  <code>Autoembedder</code>  <p>Trained Autoembedder model.</p>     Source code in <code>src/autoembedder/learner.py</code> <pre><code>def fit(\n    parameters: Dict,\n    model: Autoembedder,\n    train_dataloader: DataLoader,\n    test_dataloader: DataLoader,\n    eval_df: Optional[Union[dd.DataFrame, pd.DataFrame]] = None,\n) -&gt; Autoembedder:\n    \"\"\"This method is the general wrapper around the fitting process. It is\n    preparing the optimizer, the loss function, the trainer, the validator and\n    the evaluator. Then it attaches everything to the corresponding engines and\n    runs the training.\n\n    Args:\n        parameters (Dict[str, Any]): The parameters of the training process.\n            In the [documentation](https://chrislemke.github.io/autoembedder/#parameters) all possible parameters are listed.\n        model (Autoembedder): The model to be trained.\n        train_dataloader (torch.utils.data.DataLoader): The dataloader for the training data.\n        test_dataloader (torch.utils.data.DataLoader): The dataloader for the test data.\n        eval_df (Optional[Union[dd.DataFrame, pd.DataFrame]], optional): Dask or Pandas DataFrame for the evaluation step.\n            If the path to the evaluation data is given in the parameters (`eval_input_path`) this argument is not needed.\n            If neither `eval_input_path` nor `eval_df` is given, no evaluation step is performed.\n\n    Returns:\n        Autoembedder: Trained Autoembedder model.\n    \"\"\"\n\n    model = model.to(\n        torch.device(\n            \"cuda\"\n            if torch.cuda.is_available()\n            else \"mps\"\n            if torch.backends.mps.is_available() and parameters.get(\"use_mps\", False)\n            else \"cpu\"\n        )\n    )\n    if (\n        torch.backends.mps.is_available() is False\n        or parameters.get(\"use_mps\", False) is False\n    ):\n        model = model.double()\n\n    optimizer = Adam(\n        model.parameters(),\n        lr=parameters.get(\"lr\", 1e-3),\n        weight_decay=parameters.get(\"weight_decay\", 0),\n        amsgrad=parameters.get(\"amsgrad\", False),\n    )\n    criterion = MSELoss()\n\n    if parameters.get(\"xavier_init\", False):\n        model.init_xavier_weights()\n\n    tb_logger = None\n    if parameters.get(\"tensorboard_log_path\", None):\n        tb_logger = TensorboardLogger(\n            log_dir=f\"{parameters['tensorboard_log_path']}/{date.strftime('%Y.%m.%d-%H_%M')}\"\n        )\n\n    trainer = Engine(\n        partial(\n            __training_step,\n            model=model,\n            optimizer=optimizer,\n            criterion=criterion,\n            parameters=parameters,\n        )\n    )\n    validator = Engine(\n        partial(\n            __validation_step, model=model, criterion=criterion, parameters=parameters\n        )\n    )\n    evaluator = Engine(\n        partial(loss_delta, model=model, parameters=parameters, df=eval_df)\n    )\n\n    if parameters.get(\"verbose\", 0) &gt;= 1:\n        __print_summary(model, train_dataloader, parameters)\n    __attach_progress_bar(trainer, parameters.get(\"verbose\", False))\n    __attach_tb_logger_if_needed(\n        trainer, validator, evaluator, tb_logger, model, optimizer, parameters\n    )\n    __attach_terminate_on_nan(trainer)\n    __attach_validation(\n        trainer, validator, test_dataloader, parameters.get(\"verbose\", 0) &gt;= 1\n    )\n\n    if (\n        parameters.get(\"eval_input_path\", None) or eval_df is not None\n    ) and parameters.get(\"target\", None):\n        __attach_evaluation(\n            trainer, evaluator, test_dataloader, parameters.get(\"verbose\", 0) &gt;= 1\n        )\n    __attach_checkpoint_saving_if_needed(\n        trainer, validator, model, optimizer, parameters\n    )\n\n    __attach_tb_teardown_if_needed(tb_logger, trainer, validator, evaluator, parameters)\n\n    if parameters.get(\"load_checkpoint_path\", None):\n        checkpoint = torch.load(\n            parameters[\"load_checkpoint_path\"],\n            map_location=torch.device(\n                \"cuda\"\n                if torch.cuda.is_available()\n                else \"mps\"\n                if torch.backends.mps.is_available()\n                and parameters.get(\"use_mps\", False)\n                else \"cpu\"\n            ),\n        )\n        Checkpoint.load_objects(\n            to_load={\"model\": model, \"optimizer\": optimizer, \"trainer\": trainer},\n            checkpoint=checkpoint,\n        )\n        print(\n            f\"\"\"\n            Checkpoint loaded!\n            Epoch_length: {checkpoint['trainer']['epoch_length']}\n            Iterations: {checkpoint['trainer']['iteration']}\n            \"\"\"\n        )\n\n    trainer.run(\n        train_dataloader,\n        max_epochs=parameters[\"epochs\"],\n        epoch_length=(\n            len(train_dataloader.dataset.ddf.index) // train_dataloader.batch_size\n        ),\n    )\n    return model\n</code></pre>"},{"location":"model/","title":"Model","text":""},{"location":"model/#src.autoembedder.model.Autoembedder","title":"<code>Autoembedder</code>","text":"<p>         Bases: <code>nn.Module</code></p>  Source code in <code>src/autoembedder/model.py</code> <pre><code>class Autoembedder(nn.Module):\n    def _autoencoder(\n        self, num_cont_features: int\n    ) -&gt; Tuple[nn.Sequential, nn.Sequential]:\n        \"\"\"\n        Args:\n            num_cont_features (int): Number of continues features.\n        Returns:\n            Tuple[torch.nn.Sequential, torch.nn.Sequential]: Tuple containing the encoder and decoder.\n        \"\"\"\n\n        in_features = num_cont_features + sum(\n            emb.embedding_dim for emb in self.embeddings\n        )\n        hl = self.config[\"hidden_layers\"]\n\n        encoder_input = nn.Linear(\n            in_features, hl[0][0], bias=self.config.get(\"layer_bias\", True)\n        )\n        encoder_hidden_layers = nn.ModuleList(\n            [\n                nn.Linear(hl[x][0], hl[x][1], bias=self.config.get(\"layer_bias\", True))\n                for x in range(len(hl))\n            ]\n        )\n        decoder_hidden_layers = nn.ModuleList(\n            [\n                nn.Linear(hl[x][1], hl[x][0], bias=self.config.get(\"layer_bias\", True))\n                for x in reversed(range(len(hl)))\n            ]\n        )\n        decoder_output = nn.Linear(\n            hl[0][0], in_features, bias=self.config.get(\"layer_bias\", True)\n        )\n\n        encoder = nn.Sequential(encoder_input, *encoder_hidden_layers)\n        decoder = nn.Sequential(*decoder_hidden_layers, decoder_output)\n        return encoder, decoder\n\n    def __init__(\n        self,\n        config: Dict,\n        num_cont_features: int,\n        embedding_sizes: Optional[List[Tuple[int, int]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            config (Dict[str, Any]): Configuration for the model.\n                In the [documentation](https://chrislemke.github.io/autoembedder/#parameters) all possible parameters are listed.\n            num_cont_features (int): Number of continues features.\n            embedding_sizes (Optional[List[Tuple[int, int]]]): List of tuples.\n                Each tuple contains the size of the dictionary (unique values) of embeddings and the size of each embedding vector.\n                Only needs to be provided if categorical columns are used.\n\n        Returns:\n            None\n        \"\"\"\n        super().__init__()\n\n        if embedding_sizes is None:\n            embedding_sizes = []\n\n        self.config = config\n        self.last_target: Optional[torch.Tensor] = None\n        self.code_value: Optional[torch.Tensor] = None\n        self.embeddings = nn.ModuleList(\n            [nn.Embedding(t[0], t[1]) for t in embedding_sizes]\n        )\n        self.encoder, self.decoder = self._autoencoder(num_cont_features)\n\n        print(f\"Set model config: {config}\")\n        print(f\"Model `in_features`: {self.encoder[0].in_features}\")\n\n    def _activation(self, x: torch.Tensor) -&gt; torch.Tensor:\n        if self.config.get(\"activation\", \"tanh\") == \"tanh\":\n            return nn.Tanh()(x)\n        if self.config.get(\"activation\", \"tanh\") == \"relu\":\n            return nn.ReLU()(x)\n        if self.config.get(\"activation\", \"tanh\") == \"leaky_relu\":\n            return nn.LeakyReLU()(x)\n        if self.config.get(\"activation\", \"tanh\") == \"elu\":\n            return nn.ELU()(x)\n        raise ValueError(\n            f\"\"\"\n            Unsupported activation: `{self.config['activation']}`!.\n            Please pick one of the following: `tanh`, `relu`, `leaky_relu`, `elu`.\n            \"\"\"\n        )\n\n    def _encode(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = nn.Tanh()(self.encoder[0](x))\n        for layer in self.encoder[1:]:\n            x = self._activation(layer(x))\n            x = nn.Dropout(self.config.get(\"dropout_rate\", 0.0))(x)\n        return x\n\n    def _decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n        for layer in self.decoder[:-1]:\n            x = self._activation(layer(x))\n            x = nn.Dropout(self.config.get(\"dropout_rate\", 0.0))(x)\n        return self.decoder[-1](x)\n\n    def forward(self, x_cat: torch.Tensor, x_cont: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            x_cat (torch.Tensor): Tensor including the categorical values. Shape: [columns count, batch size]\n            x_cont (torch.Tensor): Tensor including the continues values. Shape: [columns count, batch size]\n        Returns:\n            torch.Tensor :Output of the 'Autoembedder'. It contains the concatenated and processed continues and categorical data.\n        \"\"\"\n        x_cont = rearrange(x_cont, \"c r -&gt; r c\")\n\n        x_emb = []\n        for i, layer in enumerate(self.embeddings):\n            value = x_cat[i].int()\n            try:\n                x_emb.append(layer(value))\n            except IndexError:\n                value = max(value.tolist())\n                raise IndexError(  # pylint: disable=W0707\n                    f\"\"\"\n                    There seems to be a problem with the index of the embedding layer: `index out of range in self`. The `num_embeddings`\n                    of the {i}. layer is {layer.num_embeddings}. The maximum value which should be embedded from the tensor is {value}.\n                    If the value ({value}) is bigger than the `num_embeddings` ({layer.num_embeddings})\n                    the embeddings layer can not embed the value. This could have multiple reasons:\n                    1. Check if the `--cat_columns` argument is a correct representation of the dataframe. Maybe it contain columns\n                        which are not a part of the actual dataframe.\n                    2. Check if the shape of the embeddings layer no. {i} is correct.\n                    3. Check if the correct data is passed to the model.\n                    \"\"\"\n                )\n        if self.embeddings:\n            x_emb = torch.cat(x_emb, 1)\n            x = torch.cat([x_cont, x_emb], 1)\n        else:\n            x = x_cont\n        self.last_target = (\n            x.clone().detach()\n        )  # Concatenated x values - used with the custom loss function: `AutoEmbLoss`.\n\n        x = self._encode(x)\n        self.code_value = x.clone().detach()  # Stores the values of the code layer.\n        return self._decode(x)\n\n    def init_xavier_weights(self) -&gt; None:\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n</code></pre>"},{"location":"model/#src.autoembedder.model.Autoembedder.__init__","title":"<code>__init__(config, num_cont_features, embedding_sizes=None)</code>","text":"<p>Parameters:</p>    Name Type Description Default     <code>config</code>  <code>Dict[str, Any]</code>  <p>Configuration for the model. In the documentation all possible parameters are listed.</p>  required    <code>num_cont_features</code>  <code>int</code>  <p>Number of continues features.</p>  required    <code>embedding_sizes</code>  <code>Optional[List[Tuple[int, int]]]</code>  <p>List of tuples. Each tuple contains the size of the dictionary (unique values) of embeddings and the size of each embedding vector. Only needs to be provided if categorical columns are used.</p>  <code>None</code>     <p>Returns:</p>    Type Description      <code>None</code>  <p>None</p>     Source code in <code>src/autoembedder/model.py</code> <pre><code>def __init__(\n    self,\n    config: Dict,\n    num_cont_features: int,\n    embedding_sizes: Optional[List[Tuple[int, int]]] = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        config (Dict[str, Any]): Configuration for the model.\n            In the [documentation](https://chrislemke.github.io/autoembedder/#parameters) all possible parameters are listed.\n        num_cont_features (int): Number of continues features.\n        embedding_sizes (Optional[List[Tuple[int, int]]]): List of tuples.\n            Each tuple contains the size of the dictionary (unique values) of embeddings and the size of each embedding vector.\n            Only needs to be provided if categorical columns are used.\n\n    Returns:\n        None\n    \"\"\"\n    super().__init__()\n\n    if embedding_sizes is None:\n        embedding_sizes = []\n\n    self.config = config\n    self.last_target: Optional[torch.Tensor] = None\n    self.code_value: Optional[torch.Tensor] = None\n    self.embeddings = nn.ModuleList(\n        [nn.Embedding(t[0], t[1]) for t in embedding_sizes]\n    )\n    self.encoder, self.decoder = self._autoencoder(num_cont_features)\n\n    print(f\"Set model config: {config}\")\n    print(f\"Model `in_features`: {self.encoder[0].in_features}\")\n</code></pre>"},{"location":"model/#src.autoembedder.model.Autoembedder.forward","title":"<code>forward(x_cat, x_cont)</code>","text":"<p>Parameters:</p>    Name Type Description Default     <code>x_cat</code>  <code>torch.Tensor</code>  <p>Tensor including the categorical values. Shape: [columns count, batch size]</p>  required    <code>x_cont</code>  <code>torch.Tensor</code>  <p>Tensor including the continues values. Shape: [columns count, batch size]</p>  required     <p>Returns:</p>    Type Description      <code>torch.Tensor</code>  <p>torch.Tensor :Output of the 'Autoembedder'. It contains the concatenated and processed continues and categorical data.</p>     Source code in <code>src/autoembedder/model.py</code> <pre><code>def forward(self, x_cat: torch.Tensor, x_cont: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        x_cat (torch.Tensor): Tensor including the categorical values. Shape: [columns count, batch size]\n        x_cont (torch.Tensor): Tensor including the continues values. Shape: [columns count, batch size]\n    Returns:\n        torch.Tensor :Output of the 'Autoembedder'. It contains the concatenated and processed continues and categorical data.\n    \"\"\"\n    x_cont = rearrange(x_cont, \"c r -&gt; r c\")\n\n    x_emb = []\n    for i, layer in enumerate(self.embeddings):\n        value = x_cat[i].int()\n        try:\n            x_emb.append(layer(value))\n        except IndexError:\n            value = max(value.tolist())\n            raise IndexError(  # pylint: disable=W0707\n                f\"\"\"\n                There seems to be a problem with the index of the embedding layer: `index out of range in self`. The `num_embeddings`\n                of the {i}. layer is {layer.num_embeddings}. The maximum value which should be embedded from the tensor is {value}.\n                If the value ({value}) is bigger than the `num_embeddings` ({layer.num_embeddings})\n                the embeddings layer can not embed the value. This could have multiple reasons:\n                1. Check if the `--cat_columns` argument is a correct representation of the dataframe. Maybe it contain columns\n                    which are not a part of the actual dataframe.\n                2. Check if the shape of the embeddings layer no. {i} is correct.\n                3. Check if the correct data is passed to the model.\n                \"\"\"\n            )\n    if self.embeddings:\n        x_emb = torch.cat(x_emb, 1)\n        x = torch.cat([x_cont, x_emb], 1)\n    else:\n        x = x_cont\n    self.last_target = (\n        x.clone().detach()\n    )  # Concatenated x values - used with the custom loss function: `AutoEmbLoss`.\n\n    x = self._encode(x)\n    self.code_value = x.clone().detach()  # Stores the values of the code layer.\n    return self._decode(x)\n</code></pre>"},{"location":"model/#src.autoembedder.model.embedded_sizes_and_dims","title":"<code>embedded_sizes_and_dims(train_df, test_df, col_collections)</code>","text":"<p>This method iterates over the columns of the dataframe. For every column it checks if the <code>col_collections</code> contains a list with additional columns. If this is the case the unique values are collected from both columns. Afterwards the values and dimensions are calculated.</p> <p>Parameters:</p>    Name Type Description Default     <code>train_df</code>  <code>dask.DataFrame</code>  <p>Training Dask DataFrame used to create the list of sizes and dimensions.</p>  required    <code>test_df</code>  <code>dask.DataFrame</code>  <p>Validation Dask DataFrame used to create the list of sizes and dimensions. Both dataframes will be concatenated.</p>  required    <code>col_collections</code>  <code>List[List[str]]</code>  <p>A list of lists of strings. It must contain lists of columns which include same values.</p>  required     <p>Returns:</p>    Type Description      <code>List[Tuple[int, int]]</code>  <p>List[Tuple[int, int]]: Each tuple contains the number of values and the dimensions for the corresponding embedding layer.</p>     Source code in <code>src/autoembedder/model.py</code> <pre><code>def embedded_sizes_and_dims(\n    train_df: dd.DataFrame, test_df: dd.DataFrame, col_collections: List[List[str]]\n) -&gt; List[Tuple[int, int]]:\n    \"\"\"This method iterates over the columns of the dataframe. For every column\n    it checks if the `col_collections` contains a list with additional columns.\n    If this is the case the unique values are collected from both columns.\n    Afterwards the values and dimensions are calculated.\n\n    Args:\n        train_df (dask.DataFrame): Training Dask DataFrame used to create the list of sizes and dimensions.\n        test_df (dask.DataFrame): Validation Dask DataFrame used to create the list of sizes and dimensions. Both dataframes will be concatenated.\n        col_collections (List[List[str]]): A list of lists of strings. It must contain lists of columns which include same values.\n    Returns:\n        List[Tuple[int, int]]: Each tuple contains the number of values and the dimensions for the corresponding embedding layer.\n    \"\"\"\n    assert (  # nosec\n        train_df.columns == test_df.columns\n    ).all(), \"Columns of both DataFrames must be equal!\"\n    df = dd.concat([train_df, test_df]).compute()\n    df = df.select_dtypes(include=\"category\")\n\n    unique = []\n    for column in df.columns:\n        for col_collection in col_collections:\n            if column in col_collection:\n                unique += [max(np.unique(df[col_collection].to_numpy()))]\n\n    return [(int(v + 1), int(min(50, (v + 1) // 2))) for v in unique]\n</code></pre>"},{"location":"model/#src.autoembedder.model.model_input","title":"<code>model_input(batch, parameters)</code>","text":"<p>Since the <code>Autoembedder</code> expects that the continues values and the categorical values are passed by different arguments this function splits the batch by type. It works with a batch of <code>torch.Tensor</code> and with floats and ints.</p> <p>Parameters:</p>    Name Type Description Default     <code>batch</code>  <code>NamedTuple</code>  <p>Batch provided by dataset.</p>  required    <code>parameters</code>  <code>Dict[str, Any]</code>  <p>Parameters for the model.</p>  required     <p>Returns:</p>    Type Description      <code>Tuple[torch.Tensor, Optional[torch.Tensor]]</code>  <p>Tuple[torch.Tensor, Optional[torch.Tensor]]: The first item contains the categorical values, the second item the continues values.</p>     Source code in <code>src/autoembedder/model.py</code> <pre><code>def model_input(\n    batch: NamedTuple, parameters: Dict\n) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor]]:\n    \"\"\"Since the `Autoembedder` expects that the continues values and the\n    categorical values are passed by different arguments this function splits\n    the batch by type. It works with a batch of `torch.Tensor` and with floats\n    and ints.\n\n    Args:\n        batch (NamedTuple): Batch provided by dataset.\n        parameters (Dict[str, Any]): Parameters for the model.\n    Returns:\n        Tuple[torch.Tensor, Optional[torch.Tensor]]: The first item contains the categorical values, the second item the continues values.\n    \"\"\"\n    device = torch.device(\n        \"cuda\"\n        if torch.cuda.is_available()\n        else \"mps\"\n        if torch.backends.mps.is_available() and parameters.get(\"use_mps\", False)\n        else \"cpu\"\n    )\n    cat = []\n    cont = []\n    if isinstance(batch[0], torch.Tensor):\n        for feature in batch:\n            if feature.dtype in [torch.int32, torch.int64]:\n                cat.append(feature)\n            elif feature.dtype in [torch.float32, torch.float64]:\n                if (\n                    feature.dtype == torch.float64\n                    and torch.backends.mps.is_available()\n                    and parameters.get(\"use_mps\", False)\n                ):\n                    feature = feature.to(torch.float32)\n                cont.append(feature)\n            else:\n                raise ValueError(f\"Unsupported dtype: {feature.dtype}!\")\n        if not cat:\n            return torch.empty((1, 0), dtype=torch.int8, device=device), torch.stack(\n                cont, 0\n            ).to(device)\n        return torch.stack(cat, 0).to(device), torch.stack(cont, 0).to(device)\n\n    # Used if `batch` does not contains tensors.\n    for feature in batch:\n        if isinstance(feature, int):\n            cat.append(feature)\n        elif isinstance(feature, float):\n            cont.append(feature)\n        else:\n            raise ValueError(f\"Unsupported type: {type(feature)}!\")\n\n    cat = [torch.tensor(cat)]\n    cont = [torch.tensor(cont)]\n    return torch.stack(cat, 0).to(device), torch.stack(cont, 0).to(device)\n</code></pre>"}]}