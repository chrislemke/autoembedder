An autoencoder (AE) is a type of artificial neural network used to learn [efficient codings](https://en.wikipedia.org/wiki/Feature_learning) of unlabeled data ([unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning)). The encoding is validated and refined by attempting to regenerate the input from the encoding. The autoencoder learns a representation (encoding) for a set of data, typically for [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction), by training the network to ignore insignificant data ("noise").

Variants exist, aiming to force the learned representations to assume useful properties. Examples are regularized autoencoders (_Sparse, Denoising and Contractive_), which are effective in learning representations for subsequent classification tasks, and Variational autoencoders, with applications as [generative models](https://en.wikipedia.org/wiki/Generative_model). Autoencoders are applied to many problems, from facial recognition, feature detection, anomaly detection to acquiring the meaning of words.Autoencoders are also generative models: they can randomly generate new data that is similar to the input data (training data).
<br>[Source](https://en.wikipedia.org/wiki/Autoencoder)

## Autoencoder vs. Autoembedder
The Autoembedder is simply an autoencoder which supports categorical values. For this it has some embeddings layer, which are placed before the encoder. Categorical values or columns from the DataFrame are automatically embedded first and then passed to the encoder together with the continues values. With [`embedded_sizes_and_dims`](https://chrislemke.github.io/autoembedder/autoembedder.model/#autoembedder.model.embedded_sizes_and_dims) the needed size/shape of the embedding layers is calculated. For this to work flawlessly, it is important that the `--cat_columns` defined in the [`main` function](https://chrislemke.github.io/autoembedder/autoembedder.training/#autoembedder.training.main) is set correctly.

## Next chapter: AE vs. BAE
To fight to challenges mentioned above, we want will implement a new autoencoder architecture called boosting autoencoder ensemble (BAE). In this [article](https://arxiv.org/abs/1910.09754) Savari et al. prove that an autoencoder based on the boosting idea, firstly implemented in [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost), can successfully fight overfitting by simultaneously increasing its capability of generalizing.
